name: test-qwen

resources:
  accelerators: RTX4090:1
  cloud: runpod
  disk_size: 50
  use_spot: false

workdir: .

setup: |
  echo "Installing dependencies..."
  pip install transformers torch accelerate
  
  echo "Downloading Qwen2.5-7B (smaller for testing)..."
  python3 -c "
  from transformers import AutoTokenizer, AutoModelForCausalLM
  MODEL_ID = 'Qwen/Qwen2.5-7B-Instruct'
  print(f'Downloading {MODEL_ID}...')
  tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
  model = AutoModelForCausalLM.from_pretrained(
      MODEL_ID,
      device_map='auto',
      torch_dtype='auto',
      trust_remote_code=True
  )
  print('Model ready!')
  "

run: |
  echo "Testing Qwen inference..."
  python3 -c "
  from transformers import AutoTokenizer, AutoModelForCausalLM
  import torch
  
  MODEL_ID = 'Qwen/Qwen2.5-7B-Instruct'
  print(f'Loading {MODEL_ID}...')
  
  tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
  model = AutoModelForCausalLM.from_pretrained(
      MODEL_ID,
      device_map='auto',
      torch_dtype='auto',
      trust_remote_code=True
  )
  
  # Test inference
  messages = [
      {'role': 'system', 'content': 'You are a helpful assistant.'},
      {'role': 'user', 'content': 'Write one sentence about GPU computing.'}
  ]
  
  text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
  inputs = tokenizer(text, return_tensors='pt').to(model.device)
  
  print('Generating response...')
  with torch.no_grad():
      outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.7)
  
  response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
  print(f'\nQwen response: {response}')
  print('\nâœ… Test successful!')
  "
  
  echo "GPU test complete!"
  nvidia-smi

envs:
  PYTHONUNBUFFERED: "1"
